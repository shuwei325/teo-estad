---
title: "Introducción^1^"
subtitle: " [Curso: Teoría Estadística](https://shuwei325.github.io/teo-estad)"
format: clean-revealjs
html-math-method:
  method: mathjax
  url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
author:
  - name: Shu Wei Chou Chen
    url: https://shuwei325.github.io
    orcid: 0000-0001-5495-2486
    email: shuwei.chou@ucr.ac.cr
    affiliations: 
    - name: Escuela de Estadística, Universidad de Costa Rica
      url: https://www.estadistica.ucr.ac.cr/
#date: last-modified
lang: es  # español
---

## Aspectos importantes

-   **Información importante**

    1.  [Mediación Virtual](https://mv1.mediacionvirtual.ucr.ac.cr/course/view.php?id=39199)
    2.  [Programa del curso](https://shuwei325.github.io/XS3310-I25/Programa-XS3310.pdf)
    3.  [Llenar el formulario (pendiente)](https://forms.gle/6r7XSi5EwSYdrk8D6)
    4.  [Slack](https://join.slack.com/t/teora-i25/shared_invite/zt-31gwglhsw-5lBmkz6AAndLf4kCgQIfsg)

-   **Reglamentación**

    -   [El Reglamento de Régimen Académico Estudiantil](https://www.cu.ucr.ac.cr/normativ/regimen_academico_estudiantil.pdf)
    -   [El Reglamento de Orden y Disciplina de los Estudiantes](https://www.cu.ucr.ac.cr/normativ/orden_y_disciplina.pdf)
    -   [El Reglamento de la Universidad de Costa Rica contra el Hostigamiento Sexual](https://www.cu.ucr.ac.cr/normativ/hostigamiento_sexual.pdf)
        -   [Comisión Institucional Contra el Hostigamiento Sexual](https://hostigamientosexualucr.ucr.ac.cr/)
            -   [Acerca de...](https://www.ucr.ac.cr/comision-institucional-contra-el-hostigamiento-sexual.html)
            -   [Video](https://www.youtube.com/watch?v=OiUyqC8qrnI)

# Hasta ahora en la carrera de Estadística:

¿Qué vimos en modelos probabilísticos discretos y contínuos?

<https://seeing-theory.brown.edu>

<https://www.acsu.buffalo.edu/~adamcunn/probability/probability.html>


## Hasta ahora en la carrera de Estadística:

En medio del auge de ciencia de datos, datos grandes, y tantas puertas que la tecnología nos ha abierto.

```{r, echo=FALSE, out.width="100%", fig.cap="Ciencia de Datos vs Estadística."}
knitr::include_graphics("figs/ciencia_de_datos.png")
```

¿Para qué necesitamos Teoría Estadística?

# ¿Para qué necesitamos Teoría Estadística?

-   Necesitamos entender el porqué los resultados teóricos funcionan.

-   Por ejemplo, si tomamos el promedio de un conjunto de dato,

    -   ¿Es el mejor estimador que podemos generar?
    -   ¿Qué significa mejor?
    -   ¿Puedo probar que efectivamente es el mejor?

## Conceptos importantes de la Inferencia Estadística

-   Variable aleatoria (v.a.)

-   Muestra aleatoria (m.a.)

-   Parámetro

-   Estadístico

-   Estimador <br> <br>

-   Modelos estadísticos y familias de modelos

-   Estadística paramétrica y no paramétrica <br> <br>

-   Modos de convergencia

-   Ley de los grandes números

-   Teorema del límite central

-   Estadísticos de orden

## Variable Aleatoria

Una *variable aleatoria (v.a.)* ${\displaystyle X}$ es una función real definida en el espacio de probabilidad ${\displaystyle (\Omega ,{\mathcal {A}},P)}$, asociado a un experimento aleatorio. $$X:\Omega \to \mathbb{R}.$$

**V.A. discreta**

-   Una variable aleatoria $Y$ se dice que es discreta si puede tomar un número finito o contablemente infinito de valores distintos.
-   La probabilidad de que $Y$ tome el valor $y$ es $P(Y=y)$.
-   Generalmente se representa por medio de tabla o fórmula.
-   Teorema:
    1.  $0 \leq p(y) \leq 1$ para toda $y$.
    2.  $\sum\limits_{y} p(y)=1$.

## Variable Aleatoria

-   Denote con $Y$ cualquier variable aleatoria. La **función de distribución de** $Y$, denotada por $F(y)$, es tal que $F(y)=P(Y \leq y)$ para $-\infty<y<\infty$.

-   **Propiedades de una función de distribución:** Si $F(y)$ es una función de distribución, entonces

    1.  $F(-\infty) \equiv \lim\limits _{y \rightarrow-\infty} F(y)=0$.
    2.  $F(\infty) \equiv \lim\limits _{y \rightarrow \infty} F(y)=1$.
    3.  $F(y)$ es una función no decreciente de $y$. \[Si $y_1$ y $y_2$ son cualesquiera valores de manera que $y_1<y_2$, entonces $F\left(y_1\right) \leq F\left(y_2\right)$.\]

-   **Valor esperado** $$\mu=E(Y)=\sum\limits_y y \cdot p(y).$$

-   **Variancia** $$Var(Y)=E\left[(Y-\mu)^2\right]=\sum\limits_y (y-\mu)^2\cdot p(y).$$

## Variable Aleatoria

**Ejemplo con la distribución binomial**

Recuerden que la distribución binomial tiene la siguiente función de probabilidad:

$$p(y)= \begin{cases}  \left(\begin{array}{l} n \\ y\end{array}\right) p^y q^{n-y}, \quad y=0,1,2, \ldots, n \text {  y  } 0 \leq p \leq 1, \\ 0 \quad \text{en otros casos.} \end{cases}$$

1.  Usando $n=3$ y $p=0.4$, obtenga y grafique la función de probabilidad y la función de distribución.
2.  Calcule su esperanza y variancia.

## Variable Aleatoria

**Ejemplo con la distribución binomial**

```{r}
#| echo: true
x = c(0,1,2,3)
prob = dbinom(x, size =3, prob=0.4)
acum = pbinom(x, size =3, prob=0.4)
cbind(x,prob,acum)
esperanza = sum(x*prob)
esperanza
variancia = sum((x-esperanza)^2*prob)
variancia
```

Recuerde que demostraron las fórmulas:

$$E(X)=np = 3\cdot 0.4 = 1.2$$ $$Var(X)=npq = 3\cdot 0.4 \cdot 0.6 = 0.72$$

## Variable Aleatoria

**V.A. continua**

-   Una variable aleatoria $Y$ con función de de distribución $F(y)$ se dice que es continua si $F(y)$ es continua, para $-\infty<y<\infty$.

-   Se puede deducir que $P(Y=y)=0$.

-   Sea $F(y)$ la función de distribución para una variable aleatoria continua $Y$. Entonces $f(y)$, dada por $$f(y)=\frac{d F(y)}{d y}=F^{\prime}(y)$$ siempre que exista la derivada, se denomina función de densidad de probabilidad para la variable aleatoria $Y$.

-   Inversamente, se deduce $$F(y)=\int_{-\infty}^y f(t) d t.$$

## Variable Aleatoria

-   **Propiedades**: Si $f(y)$ es una función de densidad para una variable aleatoria continua, entonces
    1.  $f(y) \geq 0$ para toda $y,-\infty<y<\infty$.
    2.  $\int_{-\infty}^{\infty} f(y) d y=1$.
-   Valor esperado

$$\mu=E(Y)=\int\limits_{-\infty}^{\infty} y \cdot f(y) dy$$

-   Variancia

$$Var(Y)=E\left[(Y-\mu)^2\right]=\int\limits_{-\infty}^{\infty} (y-\mu)^2  f(y) dy.$$

## Variable Aleatoria

**Ejemplo con la distribución beta**

Suponga que $Y$ tiene una distribución beta con parámetros $\alpha>0$ y $\beta>0$, es decir $$f(y)= \begin{cases}\frac{y^{\alpha-1}(1-y)^{\beta-1}}{B(\alpha, \beta)}, & 0 \leq y \leq 1 \\ 0, & \text { otro caso}\end{cases}$$

donde $$
B(\alpha, \beta)=\int_0^1 y^{\alpha-1}(1-y)^{\beta-1} d y=\frac{\Gamma(\alpha) \Gamma(\beta)}{\Gamma(\alpha+\beta)} .
$$

1.  Usando $\alpha=2$ y $\beta=1$, obtenga y grafique la función de densidad y la función de distribución.
2.  Obtenga su esperanza y variancia.

## Variable Aleatoria

**Ejemplo con la distribución beta**

::: columns
::: {.column width="50%"}
```{r}
#| echo: true
x = seq(-1,2,0.01)
densidad = dbeta(x, shape1 =2, shape2=1)
plot(x,densidad,type="l")
```
:::

::: {.column width="50%"}
```{r}
#| echo: true
acum = pbeta(x, shape1 =2, shape2=1)
plot(x,acum,type="l")
```
:::
:::

Recuerde que demostraron las fórmulas: $E(X)=\frac{\alpha}{\alpha + \beta} = \frac{2}{3},~~~Var(X)=\frac{\alpha \beta}{(\alpha + \beta)^2 (\alpha + \beta + 1)} = \frac{2}{3^2 4}=\frac{1}{18}.$

**Tarea:** Compruebe la esperanza y la variancia usando las integrales.

## Conceptos importantes de la Inferencia Estadística

-   **Muestra aleatoria (m.a.)**

Sean $X_{1}, X_{2}, ... , X_{n}$ un conjunto de $n$ variables aleatorias (v.a.) independientes e idénticamente distribuidas; este conjunto se denomina *muestra aleatoria* de una población infinita.

-   **Parámetro**

Es una característica de la población. Algunos parámetros de interés podría ser la media, varianza o la proporción en una población.

-   **Estadístico**

Es una función de la muestra aleatoria, $T=f\left(X_{1}, X_{2}, ... , X_{n}\right)$. Un estadístico es a su vez una variable aleatoria y como tal tiene su propia distribución, denominada distribución muestral, con sus parámetros correspondientes.

## Conceptos importantes de la Inferencia Estadística

-   **Estimador**

Cuando un estadístico, llámese $\hat{\theta}$, se utiliza para aproximar el valor de un parámetro $\theta$, entonces se acostumbra llamar a ese estadístico con el nombre de estimador.

**Notación:** $\theta$ parámetro a estimar y $\hat{\theta}$ es el estimador de $\theta$.

**Ejemplo:**

-   $\bar{X}$ es un estimador de $\mu$
-   $S^2$ es un estimador de $\sigma^2$
-   $\hat{p}$ es un estimador de $p$
-   $\hat{\theta}_1=\frac{X_1+X_n}{2}$ es un estimador de $\mu$
-   $\hat{\theta}_2=\frac{X_{(1)}+X_{(n)}}{2}$ es un estimador de $\mu$
-   ¿Cuál es la diferencia entre $\hat{\theta}_1$ y $\hat{\theta}_2$?

